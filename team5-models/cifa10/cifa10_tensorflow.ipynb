{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.8.0\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "\n",
    "import os\n",
    "\n",
    "from tensorflow.keras.layers import Dense, Flatten, Conv2D\n",
    "from tensorflow.keras import Model\n",
    "print(tf.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_DIR = './data'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Read Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['./data\\\\data_batch_1.bin', './data\\\\data_batch_2.bin', './data\\\\data_batch_3.bin', './data\\\\data_batch_4.bin', './data\\\\data_batch_5.bin']\n",
      "['./data\\\\test_batch.bin']\n"
     ]
    }
   ],
   "source": [
    "train_list = filter(lambda f: f.startswith('data_') and f.endswith('.bin'), os.listdir(DATA_DIR))\n",
    "train_path_list = list(map(lambda f: os.path.join(DATA_DIR, f), train_list))\n",
    "\n",
    "test_list = filter(lambda f: f.startswith('test_') and f.endswith('.bin'), os.listdir(DATA_DIR))\n",
    "test_path_list = list(map(lambda f: os.path.join(DATA_DIR, f), test_list))\n",
    "\n",
    "print(train_path_list)\n",
    "print(test_path_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "LABEL_BYTE = 1\n",
    "IMAGE_BYTES = 32 * 32 * 3\n",
    "RECORD_BYTES = LABEL_BYTE + IMAGE_BYTES"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _load_dataset(data_path_list):\n",
    "    def _process_record(record):\n",
    "        value = tf.io.decode_raw(record, tf.uint8)\n",
    "        label = value[0]\n",
    "        image = value[1:]\n",
    "        image = tf.reshape(image, (3, 32, 32))\n",
    "        image = tf.transpose(image, (1, 2, 0))\n",
    "        image = tf.cast(image, tf.float32)\n",
    "        image = image / 255\n",
    "        return image, label\n",
    "\n",
    "    dataset = tf.data.FixedLengthRecordDataset(\n",
    "        data_path_list,\n",
    "        RECORD_BYTES)\n",
    "    return dataset.map(_process_record)\n",
    "\n",
    "class MyModel(Model):\n",
    "    def __init__(self):\n",
    "        super(MyModel, self).__init__()\n",
    "        self.conv1 = Conv2D(32, 3, activation='relu')\n",
    "        self.flatten = Flatten()\n",
    "        self.d1 = Dense(128, activation='relu')\n",
    "        self.d2 = Dense(10, activation='softmax')\n",
    "\n",
    "    def call(self, x):\n",
    "        x = self.conv1(x)\n",
    "        x = self.flatten(x)\n",
    "        x = self.d1(x)\n",
    "        return self.d2(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create an instance of the model\n",
    "model = MyModel()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_object = tf.keras.losses.SparseCategoricalCrossentropy()\n",
    "optimizer = tf.keras.optimizers.Adam()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "@tf.function\n",
    "def train_step(images, labels):\n",
    "    with tf.GradientTape() as tape:\n",
    "        predictions = model(images)\n",
    "        loss = loss_object(labels, predictions)\n",
    "    gradients = tape.gradient(loss, model.trainable_variables)\n",
    "    optimizer.apply_gradients(zip(gradients, model.trainable_variables))\n",
    "\n",
    "test_loss = tf.keras.metrics.Mean(name='test_loss')\n",
    "test_accuracy = tf.keras.metrics.SparseCategoricalAccuracy(name='test_accuracy')\n",
    "\n",
    "@tf.function\n",
    "def test_step(images, labels):\n",
    "    predictions = model(images)\n",
    "    t_loss = loss_object(labels, predictions)\n",
    "\n",
    "    test_loss(t_loss)\n",
    "    test_accuracy(labels, predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "EPOCHS = 5\n",
    "SUMMARY_DIR = './summary'\n",
    "\n",
    "TRAIN_BATCH_SIZE = 32\n",
    "TEST_BATCH_SIZE = 32"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = _load_dataset(train_path_list).batch(TRAIN_BATCH_SIZE)\n",
    "test_dataset = _load_dataset(test_path_list).batch(TEST_BATCH_SIZE)\n",
    "\n",
    "summary_writer = tf.summary.create_file_writer(SUMMARY_DIR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "elapsed: 23.928967\n",
      "Epoch 1, Test Loss: 1.25106680393219, Test Accuracy: 55.47999572753906\n",
      "elapsed: 23.623842\n",
      "Epoch 2, Test Loss: 1.2054194211959839, Test Accuracy: 57.59000015258789\n",
      "elapsed: 24.076629\n",
      "Epoch 3, Test Loss: 1.2196234464645386, Test Accuracy: 57.970001220703125\n",
      "elapsed: 23.672711\n",
      "Epoch 4, Test Loss: 1.2204262018203735, Test Accuracy: 58.82999801635742\n",
      "elapsed: 23.587936\n",
      "Epoch 5, Test Loss: 1.3552522659301758, Test Accuracy: 57.480003356933594\n",
      "Training Finished.\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(EPOCHS):\n",
    "    start = time.time()\n",
    "    for images, labels in train_dataset:\n",
    "        train_step(images, labels)\n",
    "\n",
    "    for test_images, test_labels in test_dataset:\n",
    "        test_step(test_images, test_labels)\n",
    "\n",
    "    elapsed = time.time() - start\n",
    "    print('elapsed: %f' % elapsed)\n",
    "\n",
    "    template = 'Epoch {}, Test Loss: {}, Test Accuracy: {}'\n",
    "    print(template.format(epoch+1,\n",
    "                        test_loss.result(),\n",
    "                        test_accuracy.result()*100))\n",
    "\n",
    "    # Reset the metrics for the next epoch\n",
    "    test_loss.reset_states()\n",
    "    test_accuracy.reset_states()\n",
    "\n",
    "print('Training Finished.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save_weights('./checkpoints/cifa10_checkpoint')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
